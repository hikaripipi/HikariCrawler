import time
import csv
import math
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
from urllib.parse import quote_plus


def calculate_pages_needed(total_stores):
    """å¿…è¦ãªãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—ã™ã‚‹"""
    return math.ceil(total_stores / 50)


def scrape_cabacaba(total_stores=51):  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§51ä»¶ã‚’å–å¾—
    print(f"ğŸŒ¸ C-chan: {total_stores}ä»¶ã®åº—èˆ—æƒ…å ±ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ï¼")

    # æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰åº—èˆ—åã‚’èª­ã¿è¾¼ã‚€
    existing_names = set()
    csv_path = "/Users/hikarimac/Documents/python/crawler/kyabakyaba/first107/cabacaba_stores.csv"
    try:
        with open(csv_path, "r", encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            existing_names = {row["name"] for row in reader}
        print(f"ğŸ“š æ—¢å­˜ã®åº—èˆ—æ•°: {len(existing_names)}ä»¶")
    except FileNotFoundError:
        print("âš ï¸ æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")

    pages_needed = calculate_pages_needed(total_stores)
    print(f"ğŸ“š å¿…è¦ãªãƒšãƒ¼ã‚¸æ•°: {pages_needed}ãƒšãƒ¼ã‚¸")

    base_url = "https://www.caba2.net/tokyo/ginza/_list"
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    service = Service(executable_path="/usr/local/bin/chromedriver")
    driver = webdriver.Chrome(service=service, options=options)
    stores_data = []
    seen_names = existing_names.copy()

    try:
        for page in range(1, pages_needed + 1):
            url = f"{base_url}?page={page}"
            print(f"\nğŸ“„ ãƒšãƒ¼ã‚¸ {page} ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")

            try:
                driver.get(url)
                WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "club-top"))
                )
            except TimeoutException:
                print(f"âŒ ãƒšãƒ¼ã‚¸ {page} ã®èª­ã¿è¾¼ã¿ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
                continue

            soup = BeautifulSoup(driver.page_source, "html.parser")
            club_tops = soup.select("div.club-top")
            store_infos = soup.select("div.list-info")

            for idx, (club_top, store_info) in enumerate(zip(club_tops, store_infos)):
                if len(stores_data) >= total_stores:
                    break

                store_data = {
                    "name": "",
                    "kana": "",
                    "area": "",
                    "type": "",
                    "business_hours": "",
                    "holiday": "",
                    "budget": "",
                    "phone": "",
                    "address": "",
                    "website": "",
                    "gmap_url": "",
                    "description": "",
                }

                # åº—èˆ—åã¨èª­ã¿ä»®åã®å–å¾—
                text_wrapper = club_top.select_one("div.text-wrapper")
                if text_wrapper:
                    blog_title = text_wrapper.select_one("h2.blog-title a.link")
                    if blog_title:
                        full_name = blog_title.text.strip()
                        store_name = (
                            full_name.split(" - ")[0]
                            if " - " in full_name
                            else full_name
                        )

                        # æ—¢å­˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        if store_name in existing_names:
                            print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {store_name} (æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã™)")
                            continue

                        if " - " in full_name:
                            store_data["name"], store_data["kana"] = full_name.split(
                                " - "
                            )
                        else:
                            store_data["name"] = full_name
                        seen_names.add(store_data["name"])
                        store_data["website"] = blog_title["href"]

                    # èª¬æ˜æ–‡ã®å–å¾—
                    description_container = soup.select_one(
                        f"#list-tab-content > div > div > div.infinite-scroll > div:nth-child({idx + 1}) > "
                        "div.club-content > div.club-right > div.club-tab-container.pc > "
                        "div.club-outer-wrapper > div > div > div > section.card > div.text-wrapper"
                    )

                    if description_container:
                        title_elem = description_container.select_one("h3 a")
                        description_elem = description_container.select_one(
                            "p.description"
                        )
                        if title_elem and description_elem:
                            title = title_elem.text.strip()
                            description = description_elem.text.strip()
                            store_data["description"] = f"{title}\n{description}"

                    # ã‚¨ãƒªã‚¢ã¨åº—èˆ—ç¨®é¡ã®å–å¾—
                    area_text = text_wrapper.select_one("p.comment")
                    if area_text:
                        full_area = area_text.text.strip()
                        if "ã®" in full_area:
                            store_data["area"], store_data["type"] = full_area.split(
                                "ã®"
                            )

                # åº—èˆ—è©³ç´°æƒ…å ±ã®å–å¾—
                info_items = store_info.select("ul li")
                for item in info_items:
                    label = item.select_one("label.text")
                    value = item.select_one("span.show")

                    if label and value:
                        label_text = label.text.strip()
                        value_text = value.text.strip()

                        if "å–¶æ¥­æ™‚é–“" in label_text:
                            store_data["business_hours"] = value_text
                        elif "åº—ä¼‘æ—¥" in label_text:
                            store_data["holiday"] = value_text
                        elif "äºˆç®—ç›®å®‰" in label_text:
                            tax_info = value.select_one("span.tax-service-fee")
                            if tax_info:
                                store_data["budget"] = (
                                    f"{value_text.replace(tax_info.text, '')} {tax_info.text.strip()}"
                                )
                            else:
                                store_data["budget"] = value_text
                        elif "é›»è©±ç•ªå·" in label_text:
                            store_data["phone"] = value_text
                        elif "æ‰€åœ¨åœ°" in label_text:
                            store_data["address"] = value_text
                            search_query = f"{store_data['name']} {store_data['area']} {value_text.split(' ')[0]}"
                            encoded_query = quote_plus(search_query)
                            store_data["gmap_url"] = (
                                f"https://www.google.com/maps/search/?api=1&query={encoded_query}"
                            )

                if all(store_data[field] for field in ["name", "area", "address"]):
                    stores_data.append(store_data)
                    print(f"\nâœ¨ åº—èˆ—æƒ…å ± {len(stores_data)}:")
                    print(f"ğŸ“ åº—èˆ—å: {store_data['name']}")
                    print(f"ğŸ“– èª­ã¿ä»®å: {store_data['kana']}")
                    print(f"ğŸ¢ ã‚¨ãƒªã‚¢: {store_data['area']}")
                    print(f"ğŸ·ï¸ åº—èˆ—ç¨®é¡: {store_data['type']}")
                    print(f"ğŸ•’ å–¶æ¥­æ™‚é–“: {store_data['business_hours']}")
                    print(f"ğŸ“… å®šä¼‘æ—¥: {store_data['holiday']}")
                    print(f"ğŸ’° äºˆç®—: {store_data['budget']}")
                    print(f"ğŸ“± é›»è©±: {store_data['phone']}")
                    print(f"ğŸ  ä½æ‰€: {store_data['address']}")
                    print(f"ğŸ”— ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ: {store_data['website']}")
                    print(f"ğŸ—ºï¸ Googleãƒãƒƒãƒ—: {store_data['gmap_url']}")
                    print(f"ğŸ“ èª¬æ˜æ–‡:\n{store_data['description']}")

        # CSVã«ä¿å­˜
        output_file = "cabacaba_stores.csv"
        with open(output_file, "w", newline="", encoding="utf-8") as csvfile:
            fieldnames = [
                "name",
                "kana",
                "area",
                "type",
                "business_hours",
                "holiday",
                "budget",
                "phone",
                "address",
                "website",
                "gmap_url",
                "description",
            ]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for store in stores_data:
                writer.writerow(store)

        print(f"\nğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼ {len(stores_data)}ä»¶ã®åº—èˆ—æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
        print(f"ğŸ“ çµæœã¯ {output_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")

    finally:
        driver.quit()


if __name__ == "__main__":
    scrape_cabacaba(200)  # å–å¾—ã—ãŸã„åº—èˆ—æ•°ã‚’æŒ‡å®š
