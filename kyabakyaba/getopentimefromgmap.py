import asyncio
from playwright.async_api import async_playwright
import pandas as pd
from typing import Optional, List, Dict
import logging

# ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class GMapScraper:
    def __init__(self, max_concurrent: int = 5):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.browser = None
        self.context = None

    async def init_browser(self):
        """Playwrightãƒ–ãƒ©ã‚¦ã‚¶ã®åˆæœŸåŒ–"""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(headless=True)
        self.context = await self.browser.new_context(
            viewport={"width": 1280, "height": 800}
        )

    async def close_browser(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()

    async def get_opening_hours(self, gmap_url: str) -> Optional[str]:
        """Google Maps URLã‹ã‚‰å–¶æ¥­æ™‚é–“ã‚’å–å¾—ã—ã¦1ã¤ã®æ–‡å­—åˆ—ã«ã¾ã¨ã‚ã‚‹"""
        if not gmap_url or not isinstance(gmap_url, str):
            return None

        async with self.semaphore:
            try:
                page = await self.context.new_page()
                await page.goto(gmap_url, wait_until="networkidle")

                # å–¶æ¥­æ™‚é–“ã®æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
                await page.wait_for_selector(
                    'div[class*="fontHeadlineSmall"]', timeout=10000
                )

                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦å–¶æ¥­æ™‚é–“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º
                await page.evaluate("""() => {
                    const elements = document.querySelectorAll('div[class*="fontHeadlineSmall"]');
                    for (const element of elements) {
                        if (element.textContent.includes('å–¶æ¥­æ™‚é–“')) {
                            element.scrollIntoView();
                            break;
                        }
                    }
                }""")

                await asyncio.sleep(2)  # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã®è¡¨ç¤ºå¾…ã¡

                # å–¶æ¥­æ™‚é–“ã®æƒ…å ±ã‚’å–å¾—
                hours_info = await page.query_selector_all("tr.y0skZc")
                opening_hours_list = []

                for row in hours_info:
                    day = await row.query_selector("td.ylH6lf div")
                    time_info = await row.query_selector("td.mxowUb")

                    if day and time_info:
                        day_text = await day.inner_text()
                        time_text = await time_info.inner_text()
                        opening_hours_list.append(f"{day_text}: {time_text}")

                # å–¶æ¥­æ™‚é–“ã‚’1ã¤ã®æ–‡å­—åˆ—ã«ã¾ã¨ã‚ã‚‹
                if opening_hours_list:
                    opening_hours_text = "\n".join(opening_hours_list)
                    logger.info(f"âœ¨ å–¶æ¥­æ™‚é–“ã‚’å–å¾—: {len(opening_hours_list)}æ—¥åˆ†")
                    return opening_hours_text
                return None

            except Exception as e:
                logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                return None

            finally:
                await page.close()

    async def process_urls_batch(self, urls: List[str]) -> List[Optional[str]]:
        """URLã®ãƒãƒƒãƒå‡¦ç†"""
        tasks = [self.get_opening_hours(url) for url in urls]
        return await asyncio.gather(*tasks)


async def process_csv_file(input_csv: str, batch_size: int = 10):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦å–¶æ¥­æ™‚é–“ã‚’è¿½åŠ ã™ã‚‹"""
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        df = pd.read_csv(input_csv)

        if "gmap_url" not in df.columns:
            logger.error("âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ã«gmap_urlã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        # å–¶æ¥­æ™‚é–“ã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
        df["opening_hours"] = None

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®åˆæœŸåŒ–
        scraper = GMapScraper(max_concurrent=5)
        await scraper.init_browser()

        # ãƒãƒƒãƒå‡¦ç†
        total_rows = len(df)
        for i in range(0, total_rows, batch_size):
            batch_urls = df.iloc[i : i + batch_size]["gmap_url"].tolist()
            results = await scraper.process_urls_batch(batch_urls)

            # çµæœã‚’ä¿å­˜
            for j, result in enumerate(results):
                if i + j < total_rows:
                    df.at[i + j, "opening_hours"] = result

            logger.info(f"ğŸ“Š é€²æ—: {min(i + batch_size, total_rows)}/{total_rows}")

        # ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        await scraper.close_browser()

        # çµæœã‚’æ–°ã—ã„CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_file = input_csv.replace(".csv", "_with_hours.csv")
        df.to_csv(output_file, index=False, encoding="utf-8-sig")
        logger.info(f"\nâœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        logger.info(f"ğŸ“ çµæœã¯ {output_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")


def main():
    input_csv = "/Users/hikarimac/Documents/python/crawler/æ±äº¬å¤œã®éŠã³èª¿æŸ»ã¾ã¨ã‚ - æ–°å®¿ (3).csv"
    asyncio.run(process_csv_file(input_csv))


if __name__ == "__main__":
    main()
