import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import pandas as pd


def get_official_website(gmap_url):
    """Google Maps URLã‹ã‚‰å…¬å¼ã‚µã‚¤ãƒˆã®URLã‚’å–å¾—ã™ã‚‹"""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    service = Service(executable_path="/usr/local/bin/chromedriver")
    driver = webdriver.Chrome(service=service, options=options)

    try:
        print(f"ğŸ” ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {gmap_url}")
        driver.get(gmap_url)

        # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãƒœã‚¿ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
        wait = WebDriverWait(driver, 10)
        website_button = wait.until(
            EC.presence_of_element_located(
                (By.CSS_SELECTOR, 'a[data-item-id="authority"]')
            )
        )

        official_url = website_button.get_attribute("href")
        print(f"âœ¨ å…¬å¼ã‚µã‚¤ãƒˆç™ºè¦‹: {official_url}")
        return official_url

    except TimeoutException:
        print("âš ï¸ ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None
    except NoSuchElementException:
        print("âš ï¸ è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None
    finally:
        driver.quit()


def process_csv_file(input_csv):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦å…¬å¼ã‚µã‚¤ãƒˆURLã‚’è¿½åŠ ã™ã‚‹"""
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        df = pd.read_csv(input_csv)

        if "gmap_url" not in df.columns:
            print("âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ã«gmap_urlã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        # å…¬å¼ã‚µã‚¤ãƒˆã‚«ãƒ©ãƒ ã‚’è¿½åŠ 
        df["official_website"] = None

        # å„è¡Œã‚’å‡¦ç†
        for index, row in df.iterrows():
            print(f"\nğŸ¢ å‡¦ç†ä¸­: {row['name']}")  # 'store_name' ã‚’ 'name' ã«ä¿®æ­£

            if pd.isna(row["gmap_url"]):
                print("âš ï¸ Google Maps URLãŒç©ºã§ã™")
                continue

            official_url = get_official_website(row["gmap_url"])
            df.at[index, "official_website"] = official_url

            # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å¾…æ©Ÿ
            time.sleep(2)

        # çµæœã‚’æ–°ã—ã„CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_file = input_csv.replace(".csv", "_with_websites.csv")
        df.to_csv(output_file, index=False, encoding="utf-8-sig")
        print(f"\nâœ… å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“ çµæœã¯ {output_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")


def main():
    input_csv = "/Users/hikarimac/Documents/python/tabelog-crawler/kyabakyaba/cabacaba_stores.csv"  # å…¥åŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«å
    process_csv_file(input_csv)


if __name__ == "__main__":
    main()
